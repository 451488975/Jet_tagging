{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration from train_config_lstm.yml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Inputs': ['j1_ptrel',\n",
       "  'j1_etarot',\n",
       "  'j1_phirot',\n",
       "  'j1_erel',\n",
       "  'j1_deltaR',\n",
       "  'j1_pdgid',\n",
       "  'j_index'],\n",
       " 'Labels': ['j_g', 'j_q', 'j_w', 'j_z', 'j_t', 'j_index'],\n",
       " 'KerasModel': 'lstm_model',\n",
       " 'KerasModelRetrain': 'lstm_model_constraint',\n",
       " 'KerasLoss': 'categorical_crossentropy',\n",
       " 'L1Reg': 0.0001,\n",
       " 'L1RegR': 0.001,\n",
       " 'NormalizeInputs': 1,\n",
       " 'InputType': 'Conv1D',\n",
       " 'MaxParticles': 20}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Option = namedtuple(\"MyStruct\", \"inputModel inputFile tree config jsonModel outputDir\")\n",
    "\n",
    "options = Option(\n",
    "    inputModel = 'KERAS_lstm_model_weights.h5',\n",
    "    inputFile = '../processed-pythia82-lhc13-all-pt1-50k-r1_h022_e0175_t220_nonu_withPars_truth.z',\n",
    "    tree = 't_allpar_new',\n",
    "    config = 'train_config_lstm.yml',\n",
    "    jsonModel = 'KERAS_lstm_model.json',\n",
    "    outputDir = '/output'\n",
    ")\n",
    "\n",
    "print(\"Loading configuration from\", options.config)\n",
    "config = open(options.config, 'r')\n",
    "yamlConfig =  yaml.load(config, Loader=yaml.FullLoader)\n",
    "yamlConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20, 6)]           0         \n",
      "_________________________________________________________________\n",
      "lstm_lstm (LSTM)             (None, 20, 16)            1472      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "rnn_densef (Dense)           (None, 5)                 1605      \n",
      "=================================================================\n",
      "Total params: 3,077\n",
      "Trainable params: 3,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "json_file = open(options.jsonModel, 'r')\n",
    "model = model_from_json(json_file.read())\n",
    "json_file.close()\n",
    "\n",
    "model.load_weights(options.inputModel)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24816, 5)\n"
     ]
    }
   ],
   "source": [
    "# To use one data file:\n",
    "h5File = h5py.File(options.inputFile, 'r')\n",
    "treeArray = h5File[options.tree][()]\n",
    "\n",
    "#print(treeArray.shape)\n",
    "#print(treeArray.dtype.names)\n",
    "\n",
    "# List of features to use\n",
    "features = yamlConfig['Inputs']\n",
    "\n",
    "# List of labels to use\n",
    "labels = yamlConfig['Labels']\n",
    "\n",
    "# Convert to dataframe\n",
    "features_labels_df = pd.DataFrame(treeArray,columns=list(set(features+labels)))\n",
    "features_labels_df = features_labels_df.drop_duplicates()\n",
    "\n",
    "features_df = features_labels_df[features]\n",
    "labels_df = features_labels_df[labels]\n",
    "\n",
    "labels_df = labels_df.drop_duplicates()\n",
    "\n",
    "# Convert to numpy array \n",
    "features_val = features_df.values\n",
    "labels_val = labels_df.values\n",
    "\n",
    "if 'j_index' in features:\n",
    "    features_val = features_val[:,:-1] # drop the j_index feature\n",
    "if 'j_index' in labels:\n",
    "    labels_val = labels_val[:,:-1] # drop the j_index label\n",
    "    print(labels_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2dval = np.zeros((len(labels_df), yamlConfig['MaxParticles'], len(features)-1))\n",
    "for i in range(0, len(labels_df)):\n",
    "    features_df_i = features_df[features_df['j_index']==labels_df['j_index'].iloc[i]]\n",
    "    index_values = features_df_i.index.values\n",
    "    #features_val_i = features_val[index_values[0]:index_values[-1]+1,:-1] # drop the last feature j_index\n",
    "    features_val_i = features_val[np.array(index_values),:]\n",
    "    nParticles = len(features_val_i)\n",
    "    #print(\"before\", features_val_i[:,0])\n",
    "    features_val_i = features_val_i[features_val_i[:,0].argsort()[::-1]] # sort descending by first value (ptrel, usually)\n",
    "    #print(\"after\", features_val_i[:,0])\n",
    "    if nParticles>yamlConfig['MaxParticles']:\n",
    "        features_val_i =  features_val_i[0:yamlConfig['MaxParticles'],:]\n",
    "    else:        \n",
    "        features_val_i = np.concatenate([features_val_i, np.zeros((yamlConfig['MaxParticles']-nParticles, len(features)-1))])\n",
    "    features_2dval[i, :, :] = features_val_i\n",
    "    \n",
    "features_val = features_2dval\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features_val, labels_val, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_X_train_val = X_train_val.reshape(X_train_val.shape[0]*X_train_val.shape[1],X_train_val.shape[2])\n",
    "scaler = preprocessing.StandardScaler().fit(reshape_X_train_val)\n",
    "for p in range(X_train_val.shape[1]):\n",
    "    X_train_val[:,p,:] = scaler.transform(X_train_val[:,p,:])\n",
    "    X_test[:,p,:] = scaler.transform(X_test[:,p,:])    \n",
    "if 'j_index' in labels:\n",
    "    labels = labels[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam, loss=[yamlConfig['KerasLoss']], metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_val, y_train_val, batch_size = 1024, epochs = 100,\n",
    "                    validation_split = 0.25, shuffle = True, callbacks = None, \n",
    "                    use_multiprocessing=True, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LSTM') #Saves to local directory;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model('LSTM') #Loads from local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learningCurve(history):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(history.history['loss'], linewidth=1)\n",
    "    plt.plot(history.history['val_loss'], linewidth=1)\n",
    "    plt.title('Model Loss over Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['training sample loss','validation sample loss'])\n",
    "    #plt.savefig('Learning_curve.pdf')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningCurve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeRoc(features_val, labels_val, labels, model, outputDir='', outputSuffix=''):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    labels_pred = model.predict(features_val)\n",
    "    df = pd.DataFrame()\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    auc1 = {}\n",
    "    plt.figure(figsize=(10,8))       \n",
    "    for i, label in enumerate(labels):\n",
    "        df[label] = labels_val[:,i]\n",
    "        df[label + '_pred'] = labels_pred[:,i]\n",
    "        fpr[label], tpr[label], threshold = roc_curve(df[label],df[label+'_pred'])\n",
    "        auc1[label] = auc(fpr[label], tpr[label])\n",
    "        plt.plot(fpr[label],tpr[label],label='%s tagger, AUC = %.1f%%'%(label.replace('j_',''),auc1[label]*100.))\n",
    "    plt.plot([0, 1], [0, 1], lw=1, color='black', linestyle='--')\n",
    "    #plt.semilogy()\n",
    "    plt.xlabel(\"Background Efficiency\")\n",
    "    plt.ylabel(\"Signal Efficiency\")\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim(0.001,1.05)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.figtext(0.25, 0.90,'LSTM ROC Curve',fontweight='bold', wrap=True, horizontalalignment='right', fontsize=14)\n",
    "    #plt.figtext(0.35, 0.90,'preliminary', style='italic', wrap=True, horizontalalignment='center', fontsize=14) \n",
    "    #plt.savefig('%sROC_%s.pdf'%(outputDir, outputSuffix))\n",
    "    return labels_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = makeRoc(X_test, y_test, labels, model, outputSuffix='LSTM')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
